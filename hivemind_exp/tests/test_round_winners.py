import pytest
from unittest.mock import MagicMock, patch
from collections import defaultdict

from hivemind_exp.gsm8k.stage_utils import gsm8k_stage_data
from hivemind_exp.dht_utils import HivemindNode
from hivemind_exp.tests.fake_data import CK, QUESTION, SAMPLES


@pytest.fixture
def mock_node():
    node = MagicMock()
    node.key = "test_node"
    node.round_num = 0
    node.rewards = [1.0, 2.0, 3.0]  # Mock rewards that will be summed
    return node


@pytest.fixture
def mock_dht():
    return MagicMock()


@pytest.fixture
def mock_merged_prev_stage_datasets():
    # Create a mock for the merged_prev_stage_datasets function
    # This will be used to inject test data for the round_winners function
    with patch("hivemind_exp.gsm8k.stage_utils.merged_prev_stage_datasets") as mock:
        # Define what the mock should return
        final_stage_outputs = [
            {
                "node1": {
                    "question": QUESTION,
                    "answer": "42",  # Add the answer parameter
                    "stage3_prompt": "Test prompt for stage 3",
                    "final_agent_decision": {"decision_key": "<answer>\n42\n</answer>"}
                },
                "node2": {
                    "question": QUESTION,
                    "answer": "42",  # Add the answer parameter
                    "stage3_prompt": "Another test prompt",
                    "final_agent_decision": {"decision_key": "<answer>\n42\n</answer>"}
                },
                "node3": {
                    # This will be treated as a faulty completion in our test
                    "question": QUESTION,
                    "answer": "42",
                    "stage3_prompt": "Incomplete prompt",
                    # We'll set final_agent_decision to None to trigger the faulty completion path
                    "final_agent_decision": None
                },
                "node4": {
                    # Another faulty completion with different missing key
                    "answer": "42",
                    "final_agent_decision": {"decision_key": "<answer>\n42\n</answer>"},
                    "stage3_prompt": "Prompt with missing question",
                    # Missing question - will be added by the function
                    "question": None
                }
            }
        ]
        mock.return_value = (final_stage_outputs, final_stage_outputs)
        yield mock


def test_round_winners(mock_dht, mock_node, mock_merged_prev_stage_datasets):
    """Test the round_winners function to ensure it correctly calculates rewards and returns winners."""
    
    # Create the stage data with our mocked objects
    stage_data = gsm8k_stage_data(mock_dht, mock_node, SAMPLES, SAMPLES, check_interval=0.1, log_tag="test")
    
    # Get the round_winners function
    round_winners_fn = stage_data.round_winner_fn
    
    # Call the function with a limit
    winners = round_winners_fn(limit=2)
    
    # Verify merged_prev_stage_datasets was called
    mock_merged_prev_stage_datasets.assert_called_once()
    
    # Verify the basic parameters (excluding lambda functions which don't compare equal)
    args, kwargs = mock_merged_prev_stage_datasets.call_args
    assert args[0] == mock_dht
    assert args[1] == mock_node
    assert args[2] == mock_node.round_num
    assert args[3] == 3  # Stage 3
    assert kwargs.get('check_interval') == 0.1
    assert kwargs.get('log_tag') == 'test'
    
    # Verify the winners list contains the expected nodes
    # node1 and node2 should have rewards, while node3 and node4 should have 0 rewards due to faulty completions
    assert len(winners) <= 2  # Should respect the limit
    
    # node1 and node2 should be in the winners list as they have valid completions
    assert "node1" in winners or "node2" in winners
    
    # node3 and node4 should not be in the winners list as they have faulty completions
    if len(winners) > 1:
        assert "node3" not in winners
        assert "node4" not in winners


def test_round_winners_empty_outputs(mock_dht, mock_node):
    """Test round_winners function when there are no outputs."""
    
    # Create a mock that returns empty outputs
    with patch("hivemind_exp.gsm8k.stage_utils.merged_prev_stage_datasets") as mock:
        mock.return_value = ([], [])
        
        # Create the stage data with our mocked objects
        stage_data = gsm8k_stage_data(mock_dht, mock_node, SAMPLES, SAMPLES, check_interval=0.1)
        
        # Get the round_winners function
        round_winners_fn = stage_data.round_winner_fn
        
        # Call the function
        winners = round_winners_fn()
        
        # Verify the winners list is empty
        assert winners == []


def test_round_winners_with_custom_limit(mock_dht, mock_node, mock_merged_prev_stage_datasets):
    """Test round_winners function with different limit values."""
    
    # For this test, we'll simplify and just test with a single limit value
    # since we've already tested the main functionality in test_round_winners
    
    # Create the stage data with our mocked objects
    stage_data = gsm8k_stage_data(mock_dht, mock_node, SAMPLES, SAMPLES, check_interval=0.1)
    
    # Get the round_winners function
    round_winners_fn = stage_data.round_winner_fn
    
    # Test with a custom limit
    winners = round_winners_fn(limit=3)
    
    # Verify the limit is respected
    assert len(winners) <= 3
